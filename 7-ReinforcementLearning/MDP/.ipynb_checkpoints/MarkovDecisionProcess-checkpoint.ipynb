{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 stlye=\"font-size:3em\"> Markov Decision Process </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by : https://github.com/sachinbiradar9/Markov-Decision-Processes/blob/master/mdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load transition and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:46:12.062035Z",
     "start_time": "2019-09-07T07:46:10.965533Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Transitions = {}\n",
    "Reward = {}\n",
    "\n",
    "# Discount Factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Maximum error allowed in the utility of any state\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:48:23.336298Z",
     "start_time": "2019-09-07T07:48:23.321706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0 1)</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3 0)</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0 0)</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1 0)</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(3 1)</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  (0 1)  -0.04\n",
       "1  (3 0)  -0.04\n",
       "2  (0 0)  -0.04\n",
       "3  (1 0)  -0.04\n",
       "4  (3 1)    -10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = pd.read_csv(\"rewards.csv\", header=None)\n",
    "rewards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:48:36.213553Z",
     "start_time": "2019-09-07T07:48:36.186641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(3 0)</td>\n",
       "      <td>R</td>\n",
       "      <td>(3 0)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3 0)</td>\n",
       "      <td>R</td>\n",
       "      <td>(3 0)</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(3 0)</td>\n",
       "      <td>R</td>\n",
       "      <td>(3 1)</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3 0)</td>\n",
       "      <td>U</td>\n",
       "      <td>(3 1)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(3 0)</td>\n",
       "      <td>U</td>\n",
       "      <td>(3 0)</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1      2    3\n",
       "0  (3 0)  R  (3 0)  0.8\n",
       "1  (3 0)  R  (3 0)  0.1\n",
       "2  (3 0)  R  (3 1)  0.1\n",
       "3  (3 0)  U  (3 1)  0.8\n",
       "4  (3 0)  U  (3 0)  0.1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = pd.read_csv(\"transitions.csv\", header=None)\n",
    "transitions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:50:18.434356Z",
     "start_time": "2019-09-07T07:50:18.390190Z"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in transitions.iterrows():\n",
    "    if row[0] in Transitions:\n",
    "        if row[1] in Transitions[row[0]]:\n",
    "            Transitions[row[0]][row[1]].append((float(row[3]), row[2]))\n",
    "        else:\n",
    "            Transitions[row[0]][row[1]] = [(float(row[3]), row[2])]\n",
    "    else:\n",
    "        Transitions[row[0]] = {row[1]:[(float(row[3]),row[2])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:50:44.703636Z",
     "start_time": "2019-09-07T07:50:44.697033Z"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in rewards.iterrows():\n",
    "    Reward[row[0]] = float(row[1]) if row[1] != 'None' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T07:55:27.044495Z",
     "start_time": "2019-09-07T07:55:27.030309Z"
    }
   },
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "\n",
    "    \"\"\"\n",
    "    MDP defined by states, actions, transition model and reward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transition = {}, reward = {}, gamma = gamma):\n",
    "        \n",
    "        # Nodes from the transition model\n",
    "        self.states = transition.keys()\n",
    "        # Transitions\n",
    "        self.transition = transition\n",
    "        # Reward\n",
    "        self.reward = reward\n",
    "        # Gamma\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"\n",
    "        Reward for this state.\n",
    "        \"\"\"  \n",
    "        return self.reward[state]\n",
    "\n",
    "    def A(self, state):\n",
    "        \"\"\"\n",
    "        Set of actions that can be performed in this state\n",
    "        \"\"\"\n",
    "        return self.transition[state].keys()\n",
    "\n",
    "    def Q(self, state, action):\n",
    "        \"\"\"\n",
    "        For (state, action) return (probability, result-state) pairs.\n",
    "        \"\"\"\n",
    "        return self.transition[state][action]\n",
    "\n",
    "# Initialize the MarkovDecisionProcess object\n",
    "mdp = MarkovDecisionProcess(transition=Transitions, reward=Reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDPdef value_iteration():\n",
    "    \"\"\"\n",
    "    Solving the MDP by value iteration.\n",
    "    returns utility values for states after convergence\n",
    "    \"\"\"\n",
    "    S = mdp.states\n",
    "    A = mdp.A\n",
    "    Q = mdp.Q\n",
    "    R = mdp.R\n",
    "\n",
    "    #initialize value of all the states to 0 (this is k=0 case)\n",
    "    V1 = {s: 0 for s in states}\n",
    "    while True:\n",
    "        V = V1.copy()\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            #Bellman update, update the utility values\n",
    "            V1[s] = R(s) + gamma * max([ sum([p * V[s1] for (p, s1) in T(s, a)]) for a in actions(s)])\n",
    "            #calculate maximum difference in value\n",
    "            delta = max(delta, abs(V1[s] - V[s]))\n",
    "\n",
    "        #check for convergence, if values converged then return V\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            return V\n",
    "\n",
    "\n",
    "def best_policy(V):\n",
    "    \"\"\"\n",
    "    Given an MDP and a utility values V, determine the best policy as a mapping from state to action.\n",
    "    returns policies which is dictionary of the form {state1: action1, state2: action2}\n",
    "    \"\"\"\n",
    "    states = mdp.states\n",
    "    actions = mdp.actions\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        pi[s] = max(actions(s), key=lambda a: expected_utility(a, s, V))\n",
    "    return pi\n",
    "\n",
    "\n",
    "def expected_utility(a, s, V):\n",
    "    \"\"\"returns the expected utility of doing a in state s, according to the MDP and V.\"\"\"\n",
    "    T = mdp.T\n",
    "    return sum([p * V[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "\n",
    "#call value iteration\n",
    "V = value_iteration()\n",
    "print 'State - Value'\n",
    "for s in V:\n",
    "    print s, ' - ' , V[s]\n",
    "pi = best_policy(V)\n",
    "print '\\nOptimal policy is \\nState - Action'\n",
    "for s in pi:\n",
    "    print s, ' - ' , pi[s]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
